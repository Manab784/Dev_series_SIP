# *Blog Post 1*
# Green Solutions with Azure Machine learning

*The article covers the green solutions that can be possible with the help of azure machine learning. The blog is part of the green daily series.*

<div align="center">
<img src="greenhands.png" height="80%" width="100%"/><br/><br/>
</div>

<!-- #### ---<Starting from the issues and then ml and azure and then solutions> -->

Climate, agriculture, and oceans are some of the *green tags* where humanity is working together for reviving and solving the challenges that are already over the edge. Climate crisis, endangered species, marine lives, carbon emissions, pollution, agricultural efficiency, etc. are some of the challenges.
Climate change is the biggest challenge in front of humankind and several efforts have been recorded in past.

If we look out for technological tools & solutions that are part of those efforts, machine learning has always been the count.

<!-- From what we've seen, it seems to be a very common trend to pick up new words that we hear all around us and use them in our own conversations to feel updated ! This is very common in the field of technology too !! -->

Now, to ***really*** update ourselves, we must truly understand what the word means, right ?
With that said, let's have a walk around one amongst the busiest words of the 21st Century in the field of technology,
and you guessed it right, it is ***Machine Learning***. üåü

# What is Machine Learning ‚ùî

***Machine Learning (ML)*** belongs to the field of ***Artificial Intelligence (AI)***, which deals with predictions, learning through present data and making the machine act without being explicitly programmed.

<!-- which provides computer systems with the ability to automatically learn and improve from experience without being manually programmed>. It's as if we're trying to build a scaled-down version of the ***human brain*** or a part of it, thus providing our computer with the ability to think, learn and take decisions on its own. -->

<div align="center">
<br/><img src="Brain.jpg" height="80%" width="100%"/><br/><br/>
</div>

Now, if you've been in the field for quite some time, you would have heard of ML and related ***ML Algorithms***. Machine learning uses algorithms to identify ***patterns*** within data and those patterns are then used to create a data ***model*** that can make predictions. With increased data and experience, the results of machine learning will be much more accurate like how humans improve with more practice.

Okay, all of this seems great right ? So, where's the catch ‚ùî‚ùî 

The catch is when you've got to take immense time and effort in building an ***ML Model***. It's not always easy to gather the required ***Datasets*** to build a model, which equally agrees with the efforts of having to learn various ML Algorithms and choose the best, which then needs further optimization. There is also a lack of apt ***hardware*** on all devices needed to run heavy models and finally, the most important of these problems - Stackoverflow isn't your best friend when it comes to ML Models, as your use case might be very specific, not commonly found on the internet, ah that's difficult.

<div align="center">
<br/><img src="Problem.jpg"/><br/><br/>
</div>

Well, so where's the solution ? Don't you worry, ***Microsoft Azure Machine Learning*** has got you covered !! üõ°Ô∏è

# Azure Machine Learning üß†

Before we get started with Azure's Machine Learning Services, let's talk a little about ***Azure***.

The ***[Azure](https://azure.microsoft.com/en-us/)*** Cloud Platform provides more than ***200 products and cloud services*** designed to help you bring new solutions to life to solve today‚Äôs challenges and create the future. It helps you build, run and manage applications across multiple clouds, on-premises and at the edge, with the tools and frameworks of your choice.

Amongst the various products and services offered by Azure, the one we're helping you master is called ***Azure Machine Learning Services***. üß†

<div align="center">
<img src="Azure_Machine_Learning.png" height="80%" width="100%"/><br/><br/>
</div>

Before we deep dive into why you should consider using Azure ML in your end products, let's take a quick tour on the process of building an ML model. You would likely follow the steps mentioned below in order :- <br/>

## The Normal Way üíª

1. **Import/Create a ***Dataset***** - Based on your problem statement,  you are likely to either build a dataset or pick one up from the internet.
2. **Prepare Data** - Perform Data Augmentation, remove possible noise and preprocess data present in the dataset and sort/classify them into ***Training and Testing Data***.
3. **Build the Model** - Create an ML model with an analyzed choice of - ***Input size, Normalized inputs, No. of hidden layers, Core algorithm, Overfitting reduction procedures, Output size and a threshold Accuracy level.***
4. **Train the Model** - Train the created model with previously created training data.
5. **Validate the Model** - Use the testing data to measure the model for ***Accuracy Parameters*** based on threshold conditions.
6. **Save the Model and Deploy** - Finally, save the model you just created for further deployment on your end applications. üåü<br/>

Now that we've laid out the entire process, allow us to show you how Azure ML changes your development lifecycle ‚ùó<br/>

<div align="center">
<br/><img src="Brain_vs_brain.png" height="80%" width="100%"/><br/><br/>
</div>

## The Azure ML Way ü™ü

1. **Access Pre-Built Datasets/Import your own** - Pick up one of the prebuilt databases or import one of your own. Managing data has never been easier with ***Azure's strong cloud infrastructure***, supported by compatibility with various databases - ***Azure CosmosDB or other third party databases***.
2. **Build and Train Models** - Use ***Visual Studio Code and GitHub*** to make your task of building the model smoooooooth along with capabilities to ***Automatically train and tune accurate models***, along with built-in support for all Open-source libraries and frameworks - Scikit-learn, PyTorch, etc.
3. **Validate and Deploy** - Train and deploy models on Azure's robust infrastructure with ***Automated Pipelines and CI/CD***, as well as gain access to containers filled with Pre-built images and model repositories to share and track progress.
4. **Manage and Monitor** - Track, log, and analyze your models, backed up by a plethora of security services and error analyzers. ü•á<br/>

If we've still got out math skills right, we notice we've not only managed to reduce the stages of developing a ML Model, but have managed to do so by adding various robust and highly useful functionalities in the process ! üßÆ<br/>

Here's a statistical overview for you math geeks out there.

<div align="center">
<br/><img src="Azure_ML_Statisticals.png" height="80%" width="100%"/><br/><br/>
</div>

Now, Let's talk you through a few more features of Azure ML.

## Features of Azure ML üåü

1. **Ease of use with a morale boost** - Azure ML has been designed to boost your productivity and help you achieve the most out of yourself. Features such as ***Collaborative Notebooks, Intellisense, Easy compute and Kernel Switching alongside Visual Studio Code and GitHub thereby enabling source control*** offer a rich and immersive development experience.
2. **Plethora of ML development aids** - Azure ML offers a plethora of supporting tools such as ***Drag and Drop Machine Learning, Rapid Automation in Model creation, Model Interpretability alongside Robust and automated testing mechanisms*** and a lot more !.
3. **Security** - Azure ML provides for model training transparency to improve model reliability and help take quick actions in identification and diagnosis of errors. It also provides for a central registry to store and track model data and metadata, with further options for model development, training and testing log metrics. Not to mention the robust security services offered by Azure which has always got you and your data protected and shielded. üõ°Ô∏è
4. **Infrastructure** - Azure ML has always got you covered in case your ML model is bombarded with processing requests. To do this, it uses ***Autoscaling Compute - helps you scale up allocated resources dynamically by either sharing CPU and GPU clusters or by allocating additional resources, provided you've left a few pennies on your Azure account.***

That's a lot of features !! Now, let's look into how Azure ML finds it's way to ***Green Technologies***.

# Green solutions and Azure ML

By green solutions we mean a vast range of technological solutions that are part or can possibly facilitate the efforts and actions.

- If we look at climate actions and efforts, models can help hugely in the part of predictions with the existing data or building and deploying end- to-end pipeline for real time predictions and results.
- Like having models to predict the gaseous release in s forest, which is a result of wild fire, before it gains momentum and spread.
- In agriculture, we can use models to predict the land fertility or providing details on what should be cultivated for what soil type...

In the next few blogs in this series we will walk through some solutions and see it working with help of Azure ML services. 

---
---

# *Blog Post 2*
# Let's Prove It !
We read through our previous [blog post](https://github.com/Sara-cos/Dev_series_SIP/blob/drafts/First_Post/First_Post.md) and realized it was *All Talk and No Show*, right ‚ÅâÔ∏è

<div align="center">
	<img src="./All Talk No Show.png" height="100%" width="100%"/><br/><br/>
</div>

The list of benefits Azure ML provides over traditional ML methodologies can go on forever, but how do we gain your **Trust** on the topic at hand ‚ùì

*Well, kindly bear with us as we try to give our **Best Shot** at proving all the talk we made on the previous blog post.*

<div align="center">
	<img src="./Trust.png"/><br/><br/>
</div>

We thus bring to you, a comparison on the development procedure of a **Convolutional Neural Network (CNN)** - a ***Deep Learning Model*** (subset of Machine Learning) using two different sets of methodologies:

- ***Traditional Methodology*** - Importing **Python** libraries, **Dataset collection-preprocessing**, breaking our heads over it, etc.

- ***Azure ML Methodology*** - Easy tasks, Easier tasks, Way more Easier tasks, etc.
<br/><br/>

## Traditional Methodology üß†
Below is a step-by-step approach on how we developed a **CNN Model**, which performs the task of **Object Recognition, Classification** and **Detection**.

We have used the **[cifar10](https://www.tensorflow.org/datasets/catalog/cifar10)** Dataset to Train the Model, which provides it with the capability to recognize and classify **10 Different Classes** of objects.

<div align="center">
	<img src="./Object Recognition GitHub Repo.png" height="100%" width="100%"/><br/><br/>
</div>

[This](https://github.com/Manab784/Object-Recognition-and-Classification-System) is the link to the **GitHub Repository** containing all the required code for building the Model.

### *Step 1*

Import the required libraries. 

```
import numpy as np

from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.constraints import maxnorm
from keras.utils import np_utils
from keras.datasets import cifar10

import PIL.Image as Image
from cv2 import *
```

The most **Important** and **Noteworthy** libraries are - **[Keras](https://keras.io/), [cv2](https://pypi.org/project/opencv-python/) and [NumPy](https://numpy.org/).**


### *Step 2*

**Prepare, Pre-process** and **Filter** the **Dataset**. As the Dataset chosen here is relatively *small* compared to worldly data, this might not seem to be too much of a tedious task.

```
# Set random seed for purposes of reproducibility
seed = 21

# Loading in the data
(X_train, y_train), (X_test, y_test) = cifar10.load_data()

# normalize the inputs from 0-255 to between 0 and 1 by dividing by 255
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

X_train = X_train / 255.0
X_test = X_test / 255.0

# one hot encode outputs
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
class_num = y_test.shape[1]
```


### *Step 3*

Decide the **Dimension** of the Model (*Conv2D - a 2D Model*) and create various layers of the **Model**, namely **Input, Hidden** and **Output** layers. 

The **Input Size** of each layer is dependent on the size of the previous as well as input layers. The **Activation Function** must be selected according to the operation performed by the Model. Make sure to include a **Dropout Rate** at each layer to avoid **Overfitting** ( A Model which predicts 'too closely or exactly' to a particular set of data is said to be an Overfitted Model )

```
model = Sequential()

model.add(Conv2D(32, (3, 3), input_shape=X_train.shape[1:], padding='same'))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Conv2D(64, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Conv2D(64, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Conv2D(128, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Conv2D(256, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Flatten())
model.add(Dropout(0.2))

model.add(Dense(256, kernel_constraint=maxnorm(3)))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Dense(128, kernel_constraint=maxnorm(3)))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Dense(64, kernel_constraint=maxnorm(3)))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Dense(64, kernel_constraint=maxnorm(3)))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Dense(class_num))
model.add(Activation('softmax'))
```


### *Step 4*

Declare the number of **Epochs** you'd like to run on the Model and finally, **Compile** it !

*Don't forget to print out the **Accuracy** of your model. You'd want it to be as accurate as possible.*

Finally, **Save** your Model, for future use and integrations with other projects/applications you might build !

```
epochs = 25
optimizer = 'adam'

model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

print(model.summary())
numpy.random.seed(seed)

model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=64)

# Model evaluation
scores = model.evaluate(X_test, y_test, verbose=0)
print("Accuracy: %.2f%%" % (scores[1] * 100))

model.save('CNN_ImageProcessing.h5')
```

***Phew ! That was so much work*** ‚Äº

<div align="center">
	<img src="./Hard Work.png" height="100%" width="100%"/><br/><br/>
</div>

Now, let's look into how you'd perform the same set of tasks (way lesser and easier tasks) using **Azure ML** üéâ
<br/><br/>

## Easy (Azure ML) Methodology ü•Ç
Below is a step-by-step approach to one of the most **Difficult** ways of using the user-friendly **Azure ML** Service - via the **Command Line Interface (CLI)** , and we are confident, you will still find this easier than the previous procedure. 

***That's a bold statement to make ain't it ?***

### *Step 1*

Install the **Azure ML Extensions** for the **Azure CLI** by running the following command:

```
$ az extension add -n azure-cli-ml
```

You now have access to the Azure ML Command Line Tools via the CLI.


### *Step 2*

Create a new resource group for the **Azure ML Workspace**, by running the following command.

```
$ az group create -n ml -l southindia
$ az ml workspace create -w mldws -g ml
```

*Note: Remember to change the region to the **Azure Region** nearest to you. (Change 'southindia' to your desired region. Lookup this [Gist](https://gist.github.com/ausfestivus/04e55c7d80229069bf3bc75870630ec8) by ***ausfestivus*** if you aren't sure about the region closest to you.)*


### *Step 3*

Change to the **Working Directory** in your shell and export an Azure ML Workspace config to the disk.

```
$ az ml folder attach -w mlws -g ml -e mls
```

Finally, install the **python** extension to interact with Azure ML from within python by running the following command:

```
python3 -m pip install azure-cli azureml-sdk
```

Now, simply navigate to the **Machine Learning** workspace on the **Azure Portal** and open the **Azure Machine Learning** interface to use the Notebook Viewer provided.

To run your code on this **Environment**, click on **Compute** -> **Create Compute Instance**. When completed, an option to start a **Jupyter Notebook** pops up automatically.


### *Step 4*

**Surprise !!** üåü All the handwork's done above. All you need to do now is:

- Create a **Machine Learning** instance.
- Choose from **Trained Datasets**, **Automated ML**, **Designer Drag and Drop Model Deployment Interface** and loads more. (More about these on upcoming blogs !)
- Relax ‚ÄºÔ∏è

<div align="center">
	<img src="./Easy.png" height="100%" width="100%"/><br/><br/>
</div>

***If we're doing the right math***, not only is it way easier to create a Model using Azure ML, but the lines of code reduce drastically too !

***We'd like to call an end to this exercise of comparison and we think, Azure ML is MILES AHEAD of the traditional method of ML Model Development.*** 

Let us know what you think ! Catch you in the next one üéâ

---
---

# *Blog Post 3*
# How Automated ML helps decide Forest Cover Type?

## Let me tell a story (Fiction) üå≥
*The Green Team* spent half of an year collecting data of **forest cover types** and their geographic information (elevation, slope, solid, etc.). They want to build a machine learning model using these data, which output the suitable forest cover type with given geographic features. But **they have little programming experience, and only know basic ideas of machine learning**.  
Fortunately, they reach out for **[Azure Automated ML](https://azure.microsoft.com/en-us/services/machine-learning/automatedml/#features)**.

![Picture of Roosevelt National Forest](forest.jpg)

## What is Automated ML? ü§ñ
**[Azure Automated ML](https://azure.microsoft.com/en-us/services/machine-learning/automatedml/#features)** empowers you to build amazing machine learning models and applications rapidly without any code. It rapidly iterates over many combinations of algorithms and hyper-parameters to help you find the best model based on the metric of your choosing.

![How Automated ML works](auto-ml.jpg)

With Azure Automated ML, you will:
* Build models your way
* Control the model building process
* Improve productivity with automatic feature engineering
* Understand models better

## Let's walk through how *The Green Team* solved their problem. üêæ

*The Green Team* used **Azure Automated ML** to build a **Forest Cover Type Predictor** model. It predicts the suitable forest cover type based on geographic features.

![Azure Machine Learning Studio](az-ml-studio.png)

### Step 1: Create ***Compute Resource*** üíª

**Azure Machine Learning** is a platform for training and managing machine learning models, and we need compute to run the training process.  
*The Green Team* navigated to [Azure Machine Learning Studio](https://ml.azure.com/), and under **Compute** page, creating a **Compute Cluster** resource.

![](compute-resource.png)

### Step 2: Upload ***Dataset*** üßä

> This dataset comes from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Covertype). You can check out the website for more details.

In Azure Machine Learning, data for model training and other operations is usually encapsulated in an object called a dataset.

*The Green Team* entered the **Datasets** page, uploading their own dataset with following settings:
* **Basic info**
  * **Name**: Forest Cover Type Dataset
  * **Dataset type**: Tabular
  * **Description**: forest cover type data
* **Datastore and file selection**
  * **Select or create a datastore**: default setting
  * **Select files for your dataset**: upload [covtype.csv](./covtype.csv).
* **Settings and preview**
  * **File format**: Delimited
  * **Delimiter**: Comma
  * **Encoding**: UTF-8
  * **Column headers**: All files have same headers
  * **Skip rows**: None
  * **Dataset contains multi-line data**: Not selected
* **Schema**
  * Include all columns other than Path
  * Review the automatically detected types
* **Confirm details**
  * Do not profile the dataset after creation
  
![](datasets.png)

### Step 3: Start an Automated ML experiment üî®

The automated machine learning capability in **Azure Machine Learning** supports supervised machine learning models - in other words, models for which the training data includes known label values. You can use automated machine learning to train models for:  
* **Classification**: predicting categories or classes
* **Regression**: predicting numeric values
* **Time series forecasting**: predicting numeric values at a future point in time

As all the forest cover types is labeled as an integer from 1 to 7, this is a typical **Classification Task**. *The Green Team* create a ***New Automated ML run*** in **Automated ML** page with following settings:

* **Select dataset**:
  * Forest Cover Type Dataset
* **Configure run**:
  * **New experiment name**: predict-forest-cover-type
  * **Target column**: Cover_Type \(this is the label that the model is trained to predict\)
  * **Select compute cluster**: the compute cluster created previously
* **Select task and settings**:
  * **Task type: Classification** (the model predicts categories or classes)
  * **Enable deep learning**: selected
* **Additional configuration settings**:
  * **Primary metric**: AUC weighted
  * With other as default
* **Validation type**
  * **Train-validation split**
    * **Percentage validation of data**: 10
  * **Test dataset**: Test split (choose a percentage of the training data)
    * **Percentage test of data**: 20

![set up experiment](experiment.png)

The experiment started automatically. *The Green Team* then took a rest, waiting for the result of model training.

### Step 4: Review the best model ü•á

A few hours later, the experiment finished with the best model (other models are available as well). *The Green Team* checked this awesome model in the *Details* tab. 

![](result.png)

**Clicking the best model's name** (StandardScalerWrapper, RandomForest), they got more information -- explanations, metrics, etc. -- about the model.  

![](model-details.png)

The **top-4 important features** of model are Elevation, Hillshade_9am, Horizontal_Distance, and Hillshade_3pm. These might be the dominant features that deciding forest cover type.

![](explain.png)


In *Metrics* tab, they got more details of metric, like [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) and [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix).

![](roc.png)

![](confusion-matrix.png)

### Step 5: Deploy a model as a service üì¢

**Training model is not the end** of a machine learning task. **Deploying model** as a service making this awesome work **available to everyone**! 

In best model's detail page, *The Green Team* **Deployed it to web service** with following settings:
* Name: forest-cover-type-predictor
* Description: the end point of forest cover type predictor
* Compute type: Azure Container Instance
* Enable authentication: selected

![](deploy-1.png)

It took few minutes to deploy this model (deployment state became *Healthy*). In the **Endpoints** page, *The Green Team* get the RESTful API under **Consume** tab.

![](endpoint-consume.png)

After using API's in their web service, *The Green Team* provides **Forest Cover Type Predictor** around the world.

---
---

# *Blog Post 4*

---
---

# *Blog Post 5*
# Azure ML SDK for Python
In the last couple of blog posts, we guided you through a few ways to develop **ML Models** using **Azure ML Products & Services** such as **[Automated ML](../Third_Post/Third_Post.md)** and **[Designer](linktopost4)**.

Bear with us, as we guide you through another methodology to develop ML Models; *Data Scientists* and *AI Developers*, this one's crafted specially for you - Develop ML Models using the **Azure Machine Learning SDK for Python**.

<div>
	<img src="./Azure ML Python SDK Proccedure.png" height="100%" width="100%"/><br/><br/>
</div>

Before we guide you through an expansive procedure on the development and deployment of a ML Model using the Azure ML SDK for Python, let's talk a little about the service at hand.

## What is the Azure ML SDK for Python ‚ùì
Interacting with the Azure ML service has never been easier. The Azure ML SDK lets you interact with the service from any **Python Environment** - including **Jupyter Notebooks, Visual Studio Code** or your favorite **IDE**.


<div>
	<img src="./Azure ML.png" height="100%" width="100%"/><br/><br/>
</div>

A few features of the SDK, which make your experience *Smoooooooooth* are as follows:

- Explore, prepare and manage the lifecycle of your datasets used in machine learning experiments.
- Manage and monitor cloud resources for your machine learning experiments.
- Train models either locally or by using cloud resources.
- Capability to use the other Azure ML services within itself - **Automated ML** or **Designer**. It iterates through algorithms to find the best model for running predictions.
- Deploy web services to convert your trained models into **RESTful Services** that can be consumed in any application.

*Wow, those were a lot of features ‚ÄºÔ∏è* 

Now, sit back while we guide you through an in-depth understanding of how you'd use the Azure ML SDK for Python to build a ML Model.

## Let's Build a Model üß†
The Model we chose to build deals with **Crop Recognition**. It performs the basic task of **Identification of Crop Types**, which can be used as an integration with *Agricultural Based IoT Devices*.

The Model chosen here, is a **Convolutional Neural Network (CNN)** which falls under the category of **Deep Learning Models**, a subset of **Machine Learning**.


<div>
	<img src="./Crop Recognition System GitHub Repo.png" height="100%" width="100%"/><br/><br/>
</div>

[This](https://github.com/Manab784/Crop_Recognition_System) is the link to the **GitHub Repository** containing all the required code for building the Model.


### *Step 1*
***Installation of the Azure Machine Learning SDK for Python**

To Install the Azure ML SDK for python, run the following command on your terminal:

```
pip install azureml-core
```

*Note: Make sure your pip version is up-to date.*


### *Step 2*
***Setting up your Azure ML Workspace***

Run the following code on your terminal, to set up an Azure ML Workspace:

```
from azureml.core import Workspace
ws = Workspace.create(name='myworkspace',
                      subscription_id='<azure-subscription-id>',
                      resource_group='myresourcegroup',
                      create_resource_group=True,
                      location='southindia'
                     )
```

*Note: Change the **location** field to a desirable Azure Region near you !*

Use the same workspace in multiple environments by first writing it to a configuration JSON file. This saves your subscription, resource, and workspace name data.

```
ws.write_config(path="./file-path", file_name="ws_config.json")
```

Load your workspace by reading the configuration file.

```
from azureml.core import Workspace
ws_other_environment = Workspace.from_config(path="./file-path/ws_config.json")
```

***Add-ons (optional)*** : Having the **Azure CLI** handy always helps. Follow the steps mentioned below to download the same:

Download the latest release from [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-windows?tabs=azure-cli).


<div>
	<img src="./Azure CLI Download.png" height="100%" width="100%"/><br/><br/>
</div>

Then, add the Azure ML CLI Extension, by running the following command:

```
az extension add -n azure-cli-ml
```


### *Step 3*
***Setting up an Experiment***

An Experiment object needs to be set up (especially for integrations with **Automated ML**). To do so, perform the following steps:

```
from azureml.core.experiment import Experiment
experiment = Experiment(workspace=ws, name='test-experiment')
```

All Model Building related activities will be performed via the Experiment Object unless specified explicitly.

#### *Don't give up, You're halfway there !!*

<div>
	<img src="./Coding.jpg" height="100%" width="100%"/><br/><br/>
</div>

### *Step 4*
***Developing your Model***

This step should seem familiar if you've built ML Models before. This involves coding out the ML Model (here, CNN Model).

Type the following lines of code into a **.py** file, named according to your preference:

```
import numpy as np
import pandas as pd
import os
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.models import Mode
from PILasOPENCV import *

df = pd.read_csv('./Crop_details.csv')

Data = ImageDataGenerator(rescale=1 / 255, shear_range=0.2, horizontal_flip=True, vertical_flip=True)

Train_Data1 = Data.flow_from_directory('./kag2', target_size=(224, 224), batch_size=1)

Train_Data2 = Data.flow_from_directory('./crop_images', target_size=(224, 224), batch_size=1)

Test_Data = Data.flow_from_directory('./some_more_images', target_size=(224, 224), batch_size=1)

Model_Wrap = ResNet50(include_top=False, input_shape=(224, 224, 3))

for layers in Model_Wrap.layers:
	layers.trainable = False

Model_flat = Flatten()(Model_Wrap.output)
last_layer = Dense(5, activation='softmax')(Model_flat)

CNN_Model = Model(inputs=Model_Wrap.input, outputs=last_layer, )
CNN_Model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam(0.0001))

CNN_Model.summary()
CNN_Model.fit_generator(Train_Data1, epochs=50)
CNN_Model.evaluate(Train_Data1)
CNN_Model.evaluate_generator(Train_Data2)
CNN_Model.evaluate(Test_Data)

CNN_Model.save('CNN_Plant_Recognition.h5')
```


### *Step 5*
***Registering your Model***

Now that you've created your Model, **Register** the model in your workspace, which makes it very easy to **Manage, Download** and **Organize** your Model.

To do so, type the following lines of code:

```
from azureml.core.model import Model

model = Model.register(workspace=ws, model_path="./", model_name="CNN_Plant_Recognition.h5")
model.download(target_dir=os.getcwd()) #Downloads the Model into the pwd
```


### *Step 6*
***Deploying your Model***

With all the work done, it's time to deploy ‚ÄºÔ∏è

Use the following code to set Deployment configurations and finally, **Deploy your Model** ‚ÄºÔ∏è

```
deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1) 

service = Model.deploy(workspace = ws, 
					   name = "my_web_service", 
					   models = [model], 
					   inference_config = inference_config,
					   deployment_config = deployment_config)
```

***YAY !*** Your Model is ready to go !!

<div>
	<img src="./Celebration.png" height="100%" width="100%"/><br/><br/>
</div>

Those were a few interesting steps !! We're sure all of you can't wait to explore and test these steps on your own üéä

Let us know what you think ! Catch you in the next one üéâ

---
---

# *Blog Post 6*
# Start your Green Journey! üì¢

***Everyone can start a Machine Learning Journey in Azure for free!***

**Azure offers everyone with $200 credit to explore Azure service, including *Azure Machine Learning.*** You can build, train, and deploy machine learning models with an Azure free account. ***[Sign up your account now!](https://azure.microsoft.com/en-us/free/machine-learning/)***

If you are a **student**, you can get **$100 annual credit**. ***[Check out here!](https://azure.microsoft.com/en-us/free/students/)***

## Let's review Azure Machine Learning üëÄ

***Azure Machine Learning*** is a cloud-based service that helps you simplify some of tasks and reduce your time on preparing data, training models, and deploying predictive services.

In this blog series, we discuss:

* ***Azure Automated Machine Learning*** 
* ***Azure Machine Learning Designer***
*  ***Azure Machine Learning SDK for Python***.

Let's review these products in general.

### Azure Automated Machine Learning ü§ñ

***Azure Automated ML*** is a no-code solution for building your machine learning application.  

It iterates over many combinations of algorithms and hyper-parameters to help you find the best model based on a success metric of your choosing.

Follow ***[How Automated ML helps decide Forest Cover Type?](../Third_Post/Third_Post.md)*** to build your Automated ML application.

To learn more:

* *[Automated Machine Learning | Microsoft Azure](https://azure.microsoft.com/en-us/services/machine-learning/automatedml/)*
* *[What is automated ML? AutoML - Azure Machine Learning | Microsoft Docs](https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml)*
* *[Use automated machine learning in Azure Machine Learning - Learn | Microsoft Docs](https://docs.microsoft.com/en-us/learn/modules/use-automated-machine-learning/)*

### Azure Machine Learning Designer üìü



### Azure Machine Learning SDK for Python üíº



## Green Journey üå≥
---
---
---
